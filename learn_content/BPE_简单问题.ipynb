{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57322eb1",
   "metadata": {},
   "source": [
    "这里是回答bpe的相关问题的地方"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557bec3b",
   "metadata": {},
   "source": [
    "# Problem (train_bpe_tinystories): BPE Training on TinyStories (2 points)\n",
    "(a) Train a byte-level BPE tokenizer on the TinyStories dataset, using a maximum vocabulary size\n",
    "of 10,000. Make sure to add the TinyStories <|endoftext|> special token to the vocabulary.\n",
    "Serialize the resulting vocabulary and merges to disk for further inspection. How many hours\n",
    "and memory did training take? What is the longest token in the vocabulary? Does it make sense?\n",
    "Resource requirements: ≤30 minutes (no GPUs), ≤ 30GB RAM\n",
    "\n",
    "Hint You should be able to get under 2 minutes for BPE training using multiprocessing during\n",
    "pretokenization and the following two facts:\n",
    "\n",
    "### (a) The <|endoftext|> token delimits documents in the data files.\n",
    "\n",
    "### (b) The <|endoftext|> token is handled as a special case before the BPE merges are applied.\n",
    "\n",
    "(b) Profile your code. What part of the tokenizer training process takes the most time?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef0c8ff",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c46ec545",
   "metadata": {},
   "source": [
    "训练bpe的时候使用multiprocess进行并行化加速了，使用了两种写法，一种是mp.Pool, 但是这个程序卡死了；另一个方法是创建了一个list，里面放上每一个进程，但是会出现程序运行一会就挂起的情况，，使用top -H -p processs_numer 发现在sleeping， cpu利用率是零"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3dfad9",
   "metadata": {},
   "source": [
    "但是我使用倒排索引成功的将原本二三十分钟的训练bpe的过程压缩到了6分钟左右。原理是：我们每一次选出出现次数最多的byte pair之后，需要遍历整个文本的子序列进行更新（将相应的byte合并在一起），但是有了索引我们就能精准的修改那些拥有这个byte pair对的子序列。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045bba10",
   "metadata": {},
   "source": [
    "时间瓶颈： 对文本进行pre-token和merge的更新"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef17baa",
   "metadata": {},
   "source": [
    "# Problem (train_bpe_expts_owt): BPE Training on OpenWebText (2 points)\n",
    "(a) Train a byte-level BPE tokenizer on the OpenWebText dataset, using a maximum vocabulary\n",
    "size of 32,000. Serialize the resulting vocabulary and merges to disk for further inspection. What\n",
    "is the longest token in the vocabulary? Does it make sense?\n",
    "Resource requirements: ≤12 hours (no GPUs), ≤ 100GB RAM\n",
    "Deliverable: A one-to-two sentence response.\n",
    "(b) Compare and contrast the tokenizer that you get training on TinyStories versus OpenWebText.\n",
    "Deliverable: A one-to-two sentence response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c2b36b",
   "metadata": {},
   "source": [
    "首先是训练这个我的本机5点多一点的，根本不行。训练速度很慢，于是我在autodl上面租了一台强大的cpu跑成功了：\n",
    "训练过程有以下几点：\n",
    "-  刚开始训练的时候十分慢，因为刚开始有的pair非常非常大，首先找到pair之后对于相应地子序列地合并操作会很多\n",
    "- 随着后面整个pair_count(记录目前每一个pair出现地次数)在不断地减小, 因为一个pair合并导致相应subword2count，inverted_index地修改就会越来越少，于是就会越来越快 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4b8e76",
   "metadata": {},
   "source": [
    "# Problem (tokenizer_experiments): Experiments with tokenizers (4 points）\n",
    "(a) Sample 10 documents from TinyStories and OpenWebText. Using your previously-trained TinyS-\n",
    "tories and OpenWebText tokenizers (10K and 32K vocabulary size, respectively), encode these\n",
    "sampled documents into integer IDs. What is each tokenizer’s compression ratio (bytes/token)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeaa043a",
   "metadata": {},
   "source": [
    "这个在tokenzier那一小节可以更好地展示"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmprobe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
