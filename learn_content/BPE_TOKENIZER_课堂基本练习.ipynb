{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6704167a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.tensor([[1, 2, 3],[4,5,6],[7,8,9]])\n",
    "a.stride(0)\n",
    "a.stride(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9982e83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3])\n",
      "True\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def same_storage(a, b):\n",
    "    return a.untyped_storage().data_ptr() == b.untyped_storage().data_ptr()\n",
    "b = a[0]\n",
    "print(b)\n",
    "print(same_storage(a, b))\n",
    "c = torch.tensor([1,2,3])\n",
    "print(same_storage(a, c))\n",
    "\n",
    "view_a = a.view(1,9)\n",
    "print(same_storage(a, view_a))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d234c7",
   "metadata": {},
   "source": [
    "张量的转置会导致不连续，这个很好想，使用contiguous可以重新排序，但是copy了张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a612ee8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x= torch.tensor([[1,2,3],[4,5,6]])\n",
    "y = x.t().contiguous()\n",
    "print(x.is_contiguous())\n",
    "print(y.is_contiguous())\n",
    "same_storage(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07923d4b",
   "metadata": {},
   "source": [
    "torch einops 的操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cb7c7cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "from jaxtyping import Float\n",
    "import torch\n",
    "from einops import einsum\n",
    "x: Float[torch.Tensor, \"batch seq1 hidden\"] = torch.randn(2, 3, 4)\n",
    "y: Float[torch.Tensor, \"batch seq2 hidden\"] = torch.randn(2, 3, 4)\n",
    "\n",
    "\n",
    "z = x @ y.transpose(-1, -2)  # (batch, seq1, seq2)\n",
    "print(z.shape)  # (2, 3, 3)\n",
    "\n",
    "z_einsum = einsum(x, y, \"batch seq1 hideen, batch seq2 hidden -> batch seq1 seq2\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a59f68",
   "metadata": {},
   "source": [
    "这里学习的是einops里面的reduce操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ebf92d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 5])\n",
      "tensor([[5., 5., 5., 5.],\n",
      "        [5., 5., 5., 5.],\n",
      "        [5., 5., 5., 5.]])\n",
      "tensor([[5., 5., 5., 5.],\n",
      "        [5., 5., 5., 5.],\n",
      "        [5., 5., 5., 5.]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from einops import reduce\n",
    "x :Float[torch.Tensor,\"batch seq hidden\"] = torch.ones(3,4,5)\n",
    "print(x.shape)\n",
    "mean_1 = x.sum(dim=-1)\n",
    "mean_2 = reduce(x,\"... hid -> ...\", \"sum\")\n",
    "print(mean_1)\n",
    "print(mean_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0644946b",
   "metadata": {},
   "source": [
    "这里学习的是rearrange的操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a4c20d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 12, 64])\n"
     ]
    }
   ],
   "source": [
    "from einops import rearrange\n",
    "w: Float[torch.Tensor,' batch seq hidden'] = torch.ones(3,4,768)\n",
    "w = rearrange(w,'... (head head_dim) -> ... head head_dim', head=12)\n",
    "print(w.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad2f5a3",
   "metadata": {},
   "source": [
    "下面来学习一下tensor的flops："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "64383add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken for 10 runs: 0.000122 seconds\n",
      "FLOPS: 17580521874086.14  | 22060000000000.00 FLOPS | MFU : 0.80\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import timeit\n",
    "\n",
    "_4060Ti_FP32_TFLOPS = 22.06e+12  # 22.06 TFLOPS\n",
    "# 计算矩阵乘法的FLOPS\n",
    "def matmul_flops(m, n, p):\n",
    "    return 2 * m * n * p  # 2 * (m * n * p) 次浮点运算  \n",
    "\n",
    "x : Float[torch.Tensor, \"m n\"] = torch.randn(10, 10, device='cuda')\n",
    "y : Float[torch.Tensor, \"n p\"] = torch.randn(10, 10, device='cuda')\n",
    "\n",
    "def run(a, b):\n",
    "    a @ b\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.synchronize()\n",
    "\n",
    "t = timeit.timeit(lambda: run(x, y), number=10) / 10  # 平均每次运行时间\n",
    "print(f\"Time taken for 10 runs: {t:.6f} seconds\")\n",
    "\n",
    "print(f\"FLOPS: {matmul_flops(1024, 512, 2048) / t:.2f}  | {_4060Ti_FP32_TFLOPS:.2f} FLOPS | MFU : {matmul_flops(1024, 512, 2048) / t / _4060Ti_FP32_TFLOPS:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1171045b",
   "metadata": {},
   "source": [
    "Xavier 初始化（Glorot Initialization 就是为了解决在前向传播过程中，方差爆炸的问题，这个问题本质是输入维度对于生成值产生了印象，所以处以维度进行归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ea344010",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "input_dim = 512\n",
    "hidden_dim = 2048\n",
    "# To be extra safe, we truncate the normal distribution to [-3, 3].\n",
    "w = nn.Parameter(nn.init.trunc_normal_(torch.empty(input_dim, hidden_dim), std=1/np.sqrt(input_dim)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "46fbe1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "origin_data = np.arange(10,dtype=np.int32)\n",
    "origin_data.tofile('data.npy')\n",
    "\n",
    "\n",
    "data = np.memmap('data.npy', dtype=np.int32, mode='r', shape=(10,))\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "191233c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('layers.0.weight', 4096), ('layers.1.weight', 4096), ('final.weight', 64)]\n",
      "loss   is : 3.1030330657958984\n",
      "old state is tensor([[0.0856, 0.1169, 0.0363,  ..., 0.1039, 0.0838, 0.0706],\n",
      "        [0.0455, 0.1041, 0.0479,  ..., 0.0307, 0.0349, 0.0337],\n",
      "        [0.0963, 0.0055, 0.1020,  ..., 0.0699, 0.1210, 0.0122],\n",
      "        ...,\n",
      "        [0.0649, 0.0155, 0.1231,  ..., 0.0144, 0.0966, 0.1246],\n",
      "        [0.0433, 0.0471, 0.0882,  ..., 0.0896, 0.0504, 0.0154],\n",
      "        [0.1087, 0.0594, 0.1148,  ..., 0.0792, 0.1116, 0.1016]],\n",
      "       device='cuda:0')\n",
      "new state is tensor([[ 0.0757,  0.1070,  0.0263,  ...,  0.0941,  0.0739,  0.0607],\n",
      "        [ 0.0555,  0.1141,  0.0579,  ...,  0.0407,  0.0449,  0.0437],\n",
      "        [ 0.0863, -0.0045,  0.0920,  ...,  0.0599,  0.1110,  0.0022],\n",
      "        ...,\n",
      "        [ 0.0549,  0.0055,  0.1131,  ...,  0.0044,  0.0866,  0.1146],\n",
      "        [ 0.0533,  0.0571,  0.0982,  ...,  0.0996,  0.0604,  0.0254],\n",
      "        [ 0.0988,  0.0494,  0.1048,  ...,  0.0692,  0.1016,  0.0916]],\n",
      "       device='cuda:0')\n",
      "old state is tensor([[0.0837, 0.0421, 0.0688,  ..., 0.0597, 0.0592, 0.0964],\n",
      "        [0.0832, 0.1185, 0.0420,  ..., 0.0887, 0.0006, 0.0129],\n",
      "        [0.0067, 0.0061, 0.1052,  ..., 0.0242, 0.1102, 0.0972],\n",
      "        ...,\n",
      "        [0.0906, 0.0743, 0.0439,  ..., 0.0238, 0.0645, 0.0580],\n",
      "        [0.1210, 0.1164, 0.0156,  ..., 0.0551, 0.0411, 0.0046],\n",
      "        [0.0286, 0.1140, 0.0409,  ..., 0.0265, 0.0587, 0.0390]],\n",
      "       device='cuda:0')\n",
      "new state is tensor([[ 0.0737,  0.0321,  0.0597,  ...,  0.0497,  0.0502,  0.0864],\n",
      "        [ 0.0732,  0.1085,  0.0326,  ...,  0.0787, -0.0086,  0.0029],\n",
      "        [-0.0033, -0.0039,  0.0959,  ...,  0.0142,  0.1010,  0.0873],\n",
      "        ...,\n",
      "        [ 0.0807,  0.0646,  0.0376,  ...,  0.0138,  0.0584,  0.0481],\n",
      "        [ 0.1110,  0.1065,  0.0073,  ...,  0.0452,  0.0329, -0.0054],\n",
      "        [ 0.0186,  0.1040,  0.0315,  ...,  0.0165,  0.0494,  0.0290]],\n",
      "       device='cuda:0')\n",
      "old state is tensor([[0.0422],\n",
      "        [0.0233],\n",
      "        [0.0043],\n",
      "        [0.0163],\n",
      "        [0.0541],\n",
      "        [0.0428],\n",
      "        [0.0178],\n",
      "        [0.1046],\n",
      "        [0.0498],\n",
      "        [0.0857],\n",
      "        [0.0385],\n",
      "        [0.0729],\n",
      "        [0.0940],\n",
      "        [0.0186],\n",
      "        [0.0072],\n",
      "        [0.0627],\n",
      "        [0.0036],\n",
      "        [0.0787],\n",
      "        [0.0059],\n",
      "        [0.0646],\n",
      "        [0.0003],\n",
      "        [0.0075],\n",
      "        [0.0702],\n",
      "        [0.0870],\n",
      "        [0.0326],\n",
      "        [0.0028],\n",
      "        [0.0556],\n",
      "        [0.0805],\n",
      "        [0.0375],\n",
      "        [0.0803],\n",
      "        [0.0425],\n",
      "        [0.0510],\n",
      "        [0.0157],\n",
      "        [0.0966],\n",
      "        [0.0086],\n",
      "        [0.0662],\n",
      "        [0.1062],\n",
      "        [0.0142],\n",
      "        [0.0368],\n",
      "        [0.0017],\n",
      "        [0.1178],\n",
      "        [0.0122],\n",
      "        [0.0937],\n",
      "        [0.0701],\n",
      "        [0.0459],\n",
      "        [0.0012],\n",
      "        [0.0618],\n",
      "        [0.1137],\n",
      "        [0.0838],\n",
      "        [0.0031],\n",
      "        [0.0237],\n",
      "        [0.0942],\n",
      "        [0.0682],\n",
      "        [0.0844],\n",
      "        [0.1002],\n",
      "        [0.0679],\n",
      "        [0.0627],\n",
      "        [0.1187],\n",
      "        [0.0683],\n",
      "        [0.1035],\n",
      "        [0.0310],\n",
      "        [0.0487],\n",
      "        [0.0041],\n",
      "        [0.0420]], device='cuda:0')\n",
      "new state is tensor([[ 0.0322],\n",
      "        [ 0.0133],\n",
      "        [-0.0057],\n",
      "        [ 0.0063],\n",
      "        [ 0.0441],\n",
      "        [ 0.0328],\n",
      "        [ 0.0078],\n",
      "        [ 0.0946],\n",
      "        [ 0.0398],\n",
      "        [ 0.0757],\n",
      "        [ 0.0285],\n",
      "        [ 0.0629],\n",
      "        [ 0.0840],\n",
      "        [ 0.0086],\n",
      "        [-0.0028],\n",
      "        [ 0.0527],\n",
      "        [-0.0064],\n",
      "        [ 0.0687],\n",
      "        [-0.0041],\n",
      "        [ 0.0546],\n",
      "        [-0.0097],\n",
      "        [-0.0025],\n",
      "        [ 0.0602],\n",
      "        [ 0.0770],\n",
      "        [ 0.0226],\n",
      "        [-0.0072],\n",
      "        [ 0.0456],\n",
      "        [ 0.0705],\n",
      "        [ 0.0275],\n",
      "        [ 0.0703],\n",
      "        [ 0.0325],\n",
      "        [ 0.0410],\n",
      "        [ 0.0057],\n",
      "        [ 0.0866],\n",
      "        [-0.0014],\n",
      "        [ 0.0562],\n",
      "        [ 0.0962],\n",
      "        [ 0.0042],\n",
      "        [ 0.0268],\n",
      "        [-0.0083],\n",
      "        [ 0.1078],\n",
      "        [ 0.0022],\n",
      "        [ 0.0837],\n",
      "        [ 0.0601],\n",
      "        [ 0.0359],\n",
      "        [-0.0088],\n",
      "        [ 0.0518],\n",
      "        [ 0.1037],\n",
      "        [ 0.0738],\n",
      "        [-0.0069],\n",
      "        [ 0.0137],\n",
      "        [ 0.0842],\n",
      "        [ 0.0582],\n",
      "        [ 0.0744],\n",
      "        [ 0.0902],\n",
      "        [ 0.0579],\n",
      "        [ 0.0527],\n",
      "        [ 0.1087],\n",
      "        [ 0.0583],\n",
      "        [ 0.0935],\n",
      "        [ 0.0210],\n",
      "        [ 0.0387],\n",
      "        [-0.0059],\n",
      "        [ 0.0320]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from typing import Iterable\n",
    "import torch.nn.functional as F\n",
    "from copy import deepcopy\n",
    "\n",
    "#  realize the model\n",
    "def get_num_parameters(model:nn.Module):\n",
    "   \n",
    "   return sum(p.numel() for p in model.parameters())\n",
    "    \n",
    "class Linear(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.rand((input_dim,output_dim)) / np.sqrt(input_dim))\n",
    "    \n",
    "    def forward(self, x : torch.Tensor) -> torch.Tensor:\n",
    "        out = x @ self.weight\n",
    "        return out   \n",
    "        \n",
    "        \n",
    "class Cruncher(nn.Module):\n",
    "    def __init__(self, dim:int, num_layers:int =2):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            Linear(dim, dim)\n",
    "            for i in range(num_layers)\n",
    "        ])\n",
    "        self.final = Linear(dim, 1)\n",
    "        \n",
    "        \n",
    "    def forward(self, x: torch.Tensor)-> torch.Tensor:\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        out = self.final(x)\n",
    "        \n",
    "        out = out.squeeze(-1)\n",
    "        return out\n",
    "        \n",
    "\n",
    "# 定义adagrad优化器 \n",
    "class AdaGrad(torch.optim.Optimizer):\n",
    "    def __init__(self, params: Iterable[nn.Parameter], lr :float = 0.01):\n",
    "        super(AdaGrad, self).__init__(params, dict(lr = lr))\n",
    "        \n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            lr = group[\"lr\"]\n",
    "            for p in group[\"params\"]:\n",
    "                \n",
    "                # optimize state \n",
    "                state = self.state[p]\n",
    "                grad = p.grad.data\n",
    "                \n",
    "                # # Get squared gradients g2 \n",
    "                g2 = state.get('g2', torch.zeros_like(grad))\n",
    "                \n",
    "                # update optimizer state\n",
    "                g2 += torch.square(grad)\n",
    "                state['g2'] = g2\n",
    "                # update parameters\n",
    "                p.data -= lr * grad / torch.sqrt(g2 + 1e-5)\n",
    "\n",
    "\n",
    "\n",
    "# 初始化模型\n",
    "def custom_model(B,D):\n",
    "    seed = 2025\n",
    "    # three places\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    \n",
    "    device = torch.device(\"cuda:0\")\n",
    "    \n",
    "    \n",
    "    num_layers = 2\n",
    "    model = Cruncher(dim = D, num_layers=num_layers)\n",
    "    \n",
    "    param_sizes = [\n",
    "        (name, param.numel())\n",
    "        for name, param in model.state_dict().items()\n",
    "    ]\n",
    "    \n",
    "    print(param_sizes)\n",
    "    \n",
    "    # generate some fake data\n",
    "    model = model.to(device)\n",
    "    x = torch.randn(B, D, device= device)\n",
    "    y = model(x)\n",
    "    assert y.size() == torch.Size([B])\n",
    "    \n",
    "    \n",
    "    num_parameters = get_num_parameters(model)\n",
    "    \n",
    "    # assert num_parameters == num_layers * (D * D) + D\n",
    "\n",
    "\n",
    "    return model\n",
    "    \n",
    "    \n",
    "def optimizer():\n",
    "    B = 2\n",
    "    D = 64\n",
    "    model = custom_model(B=B, D=D).to(torch.device(\"cuda:0\"))\n",
    "    optimizer = AdaGrad(model.parameters(), lr = 0.01)\n",
    "    state = deepcopy(model.state_dict())\n",
    "\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    x = torch.randn(B, D, device=torch.device(\"cuda:0\"))\n",
    "    label = torch.tensor([3.2, 5], dtype=torch.float32, device = torch.device(\"cuda:0\"))\n",
    "    \n",
    "    y = model(x)\n",
    "    \n",
    "    loss = F.mse_loss(input = y, target = label)\n",
    "    print(f\"loss   is : {loss.item()}\")\n",
    "    loss.backward()\n",
    "    \n",
    "    \n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    new_state = model.state_dict()\n",
    "    assert state.keys() == new_state.keys()\n",
    "    for k in state.keys():\n",
    "        print(f\"old state is {state[k]}\")\n",
    "        print(f\"new state is {new_state[k]}\")\n",
    "        \n",
    "        assert not torch.equal(state[k], new_state[k])\n",
    "\n",
    "optimizer()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs336-basics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
