{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3100d430",
   "metadata": {},
   "source": [
    "# 首先我们先加载已经写好的tokenizer地代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f80290a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json\n",
    "import regex\n",
    "from typing import Iterable, Iterator, List, Dict\n",
    "from multiprocessing import Pool\n",
    "\n",
    "from cs336_basics.utils import load_bytes_dict_from_pickle, load_merges_from_pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "# copy from bpe training, 有小的改动\n",
    "def split_on_special_tokens(text: str, special_tokens: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    把 text 按照 special_tokens 拆分，返回不包含 special token 的各个子段。\n",
    "    空字符串片段会被过滤掉。\n",
    "    \"\"\"\n",
    "    # 用 re.escape 把每个 special token 转义，确保正则安全\n",
    "    escaped = [regex.escape(tok) for tok in special_tokens]\n",
    "    # 构造拆分正则：任意一个 special token\n",
    "    # 用捕获组把 matched token 保留下来（可选，看你要不要保留 token 本身）\n",
    "    pattern = \"(\" + \"|\".join(escaped) + \")\"\n",
    "    parts = regex.split(pattern, text)\n",
    "    parts = [part for part in parts if part != \"\"]\n",
    "    # parts 中包含拆分出的文本片段 & 拆分符号本身（因为用了捕获组）\n",
    "    docs: List[str] = []\n",
    "    cur: List[str] = []\n",
    "\n",
    "  \n",
    "    index = 0\n",
    "    while index < len(parts):\n",
    "        seg= parts[index]\n",
    "        if seg in special_tokens:\n",
    "            tmp = index\n",
    "            all_special_tokens = []\n",
    "            while tmp >= 0 and  tmp < len(parts) and parts[tmp] in special_tokens:\n",
    "                all_special_tokens.append(parts[tmp])\n",
    "                tmp += 1\n",
    "            i = len(all_special_tokens)\n",
    "            while i >= 1:\n",
    "                if ''.join(all_special_tokens[:i]) in special_tokens:\n",
    "                    docs.append(\"\".join(cur))\n",
    "                    docs.append(''.join(all_special_tokens[:i]))\n",
    "                    cur = []\n",
    "                    break\n",
    "                i -= 1\n",
    "            index = index + i\n",
    "        else:\n",
    "            cur.append(seg)\n",
    "            index +=1\n",
    "    # 最后剩下的也算一个 doc\n",
    "    if cur:\n",
    "        docs.append(\"\".join(cur))\n",
    "    \n",
    "    # 过滤空字符串\n",
    "    docs = [d for d in docs if d != \"\"]\n",
    "    # print(f\"docs is {docs}\")\n",
    "    return docs\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "    def __init__(self, \n",
    "                 vocab: dict[int, bytes],\n",
    "                 merges: list[tuple[bytes, bytes]],\n",
    "                 special_tokens: list[str] | None = None):\n",
    "        \n",
    "        # 保存的关键数据结构\n",
    "        self.id2token = {}\n",
    "        self.token2id = {}\n",
    "        \n",
    "        self.vocab = vocab\n",
    "        \n",
    "        for i, token in vocab.items():\n",
    "            self.id2token[i] = token\n",
    "            self.token2id[token] = i\n",
    "        assert self.id2token == self.vocab, \"id2token and vocab are not the same\"\n",
    "        \n",
    "        self.bpe_ranks = merges\n",
    "        self.special_tokens = special_tokens\n",
    "        self.PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "        \n",
    "        \n",
    "        # pprint.pprint(self.token2id)\n",
    "        # 处理special tokens\n",
    "        if special_tokens:\n",
    "            for special_token in special_tokens:\n",
    "                if special_token.encode(\"utf-8\") not in self.token2id.keys():\n",
    "                    self.vocab[len(self.vocab)] = special_token.encode(\"utf-8\")\n",
    "                    self.token2id[special_token.encode(\"utf-8\")] = len(self.token2id)\n",
    "                    self.id2token[len(self.id2token)] = special_token.encode(\"utf-8\")\n",
    "        \n",
    "        if special_tokens:\n",
    "            self.special_tokens_bytes = [special_token.encode(\"utf-8\") for special_token in special_tokens]\n",
    "        else:\n",
    "            self.special_tokens_bytes = None\n",
    "        \n",
    "    @classmethod\n",
    "    def from_files(cls, vocab_filepath: str,\n",
    "                   merges_filepath: str,\n",
    "                   special_tokens: list[str] | None = None):\n",
    "        '''\n",
    "            Class method that constructs and return a Tokenizer from a serialized vocabulary and list of merges\n",
    "            (in the same format that your BPE training code output) and (optionally) a list of special tokens\n",
    "            \n",
    "        Args:\n",
    "            vocab_filepath: path to the vocabulary file\n",
    "            merges_filepath: path to the merges file\n",
    "            special_tokens: list of special tokens\n",
    "        Returns:\n",
    "            A Tokenizer object\n",
    "        '''\n",
    "        \n",
    "         # 读取 vocab 文件\n",
    "        vocab: dict[int, bytes] = load_bytes_dict_from_pickle(vocab_filepath)\n",
    "\n",
    "        # 读取 merges 文件\n",
    "        merges : list[tuple[bytes, bytes]] = load_merges_from_pickle(merges_filepath)\n",
    "    \n",
    "\n",
    "        return cls(vocab=vocab, merges=merges, special_tokens=special_tokens)\n",
    "    \n",
    "    \n",
    "    def _get_token_byte_ids(self, token_subwords :List[bytes]) -> List[int]:\n",
    "        \n",
    "        token_byte_ids = []\n",
    "        try:\n",
    "            # 如果只有一个byte token\n",
    "            if len(token_subwords) == 1:\n",
    "                if self.token2id.get(token_subwords[0], None) is not None:\n",
    "                    token_byte_ids.append(self.token2id[token_subwords[0]])\n",
    "                    return token_byte_ids\n",
    "                else:\n",
    "                    raise ValueError(f\"Token {token_subwords[0]} not found in vocabulary\")\n",
    "            \n",
    "            has_merge_operations = True\n",
    "            while has_merge_operations == True and len(token_subwords) > 1:\n",
    "                \n",
    "                has_merge_operations = False\n",
    "                for pri_pairs in self.bpe_ranks:\n",
    "                    \n",
    "                    is_merged = False\n",
    "                    for index, (b1, b2) in enumerate(zip(token_subwords, token_subwords[1:])):\n",
    "                        if (b1, b2) == pri_pairs:\n",
    "                            # 进行替换\n",
    "                            token_subwords[index] = pri_pairs[0] + pri_pairs[1]\n",
    "                            del token_subwords[index + 1]\n",
    "                            is_merged = True\n",
    "                            break\n",
    "                    if is_merged == True:\n",
    "                        has_merge_operations = True\n",
    "                        break \n",
    "                    \n",
    "            for byte_token in token_subwords:\n",
    "                token_byte_ids.append(self.token2id[byte_token])\n",
    "            return token_byte_ids\n",
    "        except ValueError as e:\n",
    "            raise ValueError(f\"Error getting token byte ids: {e}\")\n",
    "    def encode(self, text: str) -> list[int]:\n",
    "        '''\n",
    "                Encode an input text into a sequence of token IDs\n",
    "        '''\n",
    "        tokenized_text :List[int] = []\n",
    "        \n",
    "        # splited_text 里面包含特殊的字符\n",
    "        if self.special_tokens is not None:\n",
    "            splited_text = split_on_special_tokens(text=text, special_tokens=self.special_tokens)\n",
    "        else:\n",
    "            splited_text = [text]\n",
    "        pbar = tqdm(splited_text, desc=\"Encoding text...\")\n",
    "        for text in pbar:\n",
    "            # print(f\"text: {text}\")\n",
    "            # special_tokens已经在self.token2id中，所以直接append\n",
    "            if self.special_tokens and text in self.special_tokens:\n",
    "                tokenized_text.append(self.token2id[text.encode(\"utf-8\")])\n",
    "            else:\n",
    "                for token in regex.finditer(self.PAT, text):\n",
    "                    token_str = token.group(0)\n",
    "                    token_bytes = token_str.encode(\"utf-8\")\n",
    "                    token_subwords = [token_bytes[i:i+1] for i in range(len(token_bytes))]\n",
    "                    token_byte_ids = self._get_token_byte_ids(token_subwords)\n",
    "                    tokenized_text.extend(token_byte_ids)\n",
    "            pbar.update(1)\n",
    "        return tokenized_text\n",
    "    \n",
    "    \n",
    "    def _iter_encode(self, iterable: Iterable[str]) -> Iterator[int]:\n",
    "        for index, text in enumerate(iterable):\n",
    "            encode_list = self.encode(text)\n",
    "            for id in encode_list:\n",
    "                yield id\n",
    "    \n",
    "    def encode_iterable(self, iterable: Iterable[str]) -> Iterator[int]:\n",
    "        '''\n",
    "        Encode an iterable of input texts into a sequence of token IDsGiven an iterable of\n",
    "        strings (e.g., a Python file handle), return a generator that lazily yields token IDs. This is\n",
    "        required for memory-eﬀicient tokenization of large files that we cannot directly load into\n",
    "        memory.\n",
    "        '''\n",
    "        iter_func = self._iter_encode(iterable=iterable)\n",
    "        return iter_func\n",
    "            \n",
    "    def decode(self, ids: list[int]) -> str:\n",
    "        '''\n",
    "            Decode a sequence of token IDs into a string\n",
    "        '''\n",
    "        result_component = []\n",
    "        error_bytes = b'\\x80'\n",
    "        \n",
    "        bytes_content = b''\n",
    "         \n",
    "        for id in ids:\n",
    "            if id in self.id2token:\n",
    "                token_bytes = self.id2token[id]\n",
    "                if self.special_tokens_bytes is not None and token_bytes in self.special_tokens_bytes:\n",
    "                    \n",
    "                    if bytes_content != b'':\n",
    "                        str_content = bytes_content.decode(\"utf-8\", errors=\"replace\")\n",
    "                        result_component.append(str_content)\n",
    "                        \n",
    "                    result_component.append(token_bytes.decode(\"utf-8\", errors=\"replace\"))\n",
    "                    bytes_content = b''\n",
    "                else:\n",
    "                    bytes_content += token_bytes\n",
    "            else:\n",
    "                # 畸形字节 \\x80\n",
    "                bytes_content += error_bytes\n",
    "        \n",
    "\n",
    "        if bytes_content != b'':\n",
    "            tail_str_content = bytes_content.decode(\"utf-8\", errors=\"replace\")\n",
    "            result_component.append(tail_str_content)\n",
    "        \n",
    "        return ''.join(result_component)\n",
    "\n",
    "    \n",
    "    \n",
    "    def _encode_chunk(self, sub_texts: List[str]) -> List[int]:\n",
    "        \"\"\"\n",
    "        辅助函数：对 splited_text 的一个子列表进行 encode，返回其 token ID 列表\n",
    "        \"\"\"\n",
    "        tokenized_chunk: List[int] = []\n",
    "        pbar = tqdm(sub_texts, desc=f\"sub processs Encoding chunk...\")\n",
    "        for sub_text in pbar:\n",
    "            if self.special_tokens and sub_text in self.special_tokens:\n",
    "                tokenized_chunk.append(self.token2id[sub_text.encode(\"utf-8\")])\n",
    "            else:\n",
    "                for token in regex.finditer(self.PAT, sub_text):\n",
    "                    token_str = token.group(0)\n",
    "                    token_bytes = token_str.encode(\"utf-8\")\n",
    "                    token_subwords = [token_bytes[i:i+1] for i in range(len(token_bytes))]\n",
    "                    token_byte_ids = self._get_token_byte_ids(token_subwords)\n",
    "                    tokenized_chunk.extend(token_byte_ids)\n",
    "            pbar.update(1)\n",
    "        return tokenized_chunk\n",
    "    \n",
    "    def encode_parallel(self, text: str, num_processes: int = 2) -> List[int]:\n",
    "        \"\"\"\n",
    "        并行版 encode：将 splited_text 分成 num_processes 个子块，用 multiprocessing.Pool 并发处理，\n",
    "        最后将所有子块的结果拼接起来（按顺序）。\n",
    "        \"\"\"\n",
    "        if self.special_tokens is not None:\n",
    "            splited_text = split_on_special_tokens(text=text, special_tokens=self.special_tokens)\n",
    "        else:\n",
    "            splited_text = [text]\n",
    "\n",
    "        # 分块\n",
    "        chunk_size = (len(splited_text) + num_processes - 1) // num_processes  # 向上取整\n",
    "        chunks: List[List[str]] = [\n",
    "            splited_text[i * chunk_size : (i + 1) * chunk_size]\n",
    "            for i in range(num_processes)\n",
    "            if i * chunk_size < len(splited_text)\n",
    "        ]\n",
    "\n",
    "        # 多进程池处理\n",
    "        with Pool(processes=num_processes) as pool:\n",
    "            # 注意：Pool 里的函数必须是可 picklable，全局可见\n",
    "            results = pool.map(self._encode_chunk, chunks)\n",
    "        # 拼接各子结果（按 chunks 原始顺序）\n",
    "        tokenized_text: List[int] = []\n",
    "        for res in results:\n",
    "            tokenized_text.extend(res)\n",
    "\n",
    "        return tokenized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8993f405",
   "metadata": {},
   "source": [
    "# 下面首先我们回答bpe章节最后几个问题"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb48415c",
   "metadata": {},
   "source": [
    "# Problem (tokenizer_experiments): Experiments with tokenizers (4 points）\n",
    "(a) Sample 10 documents from TinyStories and OpenWebText. Using your previously-trained TinyS-\n",
    "tories and OpenWebText tokenizers (10K and 32K vocabulary size, respectively), encode these\n",
    "sampled documents into integer IDs. What is each tokenizer’s compression ratio (bytes/token)?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d11cbc",
   "metadata": {},
   "source": [
    "#  测试tiny_story_tokenizer 在tinystory数据集上面的压缩率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b80d1056",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sub processs Encoding chunk...:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sub processs Encoding chunk...: 100%|██████████| 6/6 [00:00<00:00, 27.82it/s]\n",
      "sub processs Encoding chunk...: 100%|██████████| 5/5 [00:00<00:00, 22.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compression_ratio is 4.194475138121547\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "Tiny_story_test_path = \"/home/niu/code/cs336/assignment1-basics/tests/fixtures/tinystories_sample.txt\"\n",
    "\n",
    "Tiny_story_vocab_path = \"/home/niu/code/cs336/assignment1-basics/cs336_basics/bpe/output/TinyStoriesV2-GPT4-train_optim_vocab_10000.pkl\"\n",
    "Tiny_story_merges_path = \"/home/niu/code/cs336/assignment1-basics/cs336_basics/bpe/output/TinyStoriesV2-GPT4-train_optim_merges_10000.pkl\"\n",
    "\n",
    "tiny_story_tokenizer = Tokenizer.from_files(\n",
    "    vocab_filepath=Tiny_story_vocab_path,\n",
    "    merges_filepath=Tiny_story_merges_path,\n",
    "    special_tokens=[\"<|endoftext|>\"],\n",
    ")\n",
    "\n",
    "tiny_story_test = open(Tiny_story_test_path, \"r\", encoding=\"utf-8\").read()  \n",
    "para_tiny_story_ids = tiny_story_tokenizer.encode_parallel(tiny_story_test)\n",
    "compression_ratio = len(list(tiny_story_test.encode(\"utf-8\"))) / len(para_tiny_story_ids)\n",
    "print(f\"compression_ratio is {compression_ratio}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2d22ea",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "926ca456",
   "metadata": {},
   "source": [
    "# 测试owt训练出来的tokenizer在owt数据集上面的压缩率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83c03bd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sub processs Encoding chunk...: 100%|██████████| 1/1 [00:00<00:00, 7724.32it/s]\n",
      "sub processs Encoding chunk...: 100%|██████████| 1/1 [00:03<00:00,  3.99s/it]\n",
      "sub processs Encoding chunk...: 100%|██████████| 1/1 [00:12<00:00, 12.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " owt tokenizer on owt dataset compression_ratio is 4.427510917030568\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "owt_test_path = \"/home/niu/code/cs336/assignment1-basics/data/owt_valid.txt\"\n",
    "\n",
    "owt_vocab_path = \"/home/niu/code/cs336/assignment1-basics/cs336_basics/bpe/output/owt-train_optim_vocab_32000.pkl\"\n",
    "owt_merges_path = \"/home/niu/code/cs336/assignment1-basics/cs336_basics/bpe/output/owt-train_optim_merges_32000.pkl\"\n",
    "\n",
    "owt_tokenizer = Tokenizer.from_files(\n",
    "    vocab_filepath=owt_vocab_path,\n",
    "    merges_filepath=owt_merges_path,\n",
    "    special_tokens=[\"<|endoftext|>\"],\n",
    ")\n",
    "\n",
    "with open(owt_test_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    owt_test = f.read(20000)  # 只读取前500个字符\n",
    "para_owt_ids = owt_tokenizer.encode_parallel(owt_test, os.cpu_count())\n",
    "compression_ratio = len(list(owt_test.encode(\"utf-8\"))) / len(para_owt_ids)\n",
    "print(f\" owt tokenizer on owt dataset compression_ratio is {compression_ratio}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61bbb044",
   "metadata": {},
   "source": [
    "# 测试在tiny-story数据集上面训练的tokenzier能不能在owt的数据集上面的压缩率会不会有问题"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d699a6",
   "metadata": {},
   "source": [
    "经过简单的测试，发现这种混用会一定程度降低tokenizer的压缩率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fec40a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sub processs Encoding chunk...: 100%|██████████| 1/1 [00:00<00:00, 5622.39it/s]\n",
      "sub processs Encoding chunk...: 100%|██████████| 1/1 [00:00<00:00, 10433.59it/s]\n",
      "sub processs Encoding chunk...: 100%|██████████| 1/1 [00:00<00:00, 13.24it/s]\n",
      "sub processs Encoding chunk...: 100%|██████████| 1/1 [00:00<00:00,  5.36it/s]\n",
      "sub processs Encoding chunk...: 100%|██████████| 1/1 [00:00<00:00,  1.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " owt tokenizer on tiny story dataset compression_ratio is 3.9761431411530817\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sub processs Encoding chunk...: 100%|██████████| 1/1 [00:01<00:00,  1.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " tiny_story tokenizer on owt dataset compression_ratio is 2.9067431850789096\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "tiny_story_test_path = \"/home/niu/code/cs336/assignment1-basics/tests/fixtures/tinystories_sample_5M.txt\"\n",
    "\n",
    "owt_vocab_path = \"/home/niu/code/cs336/assignment1-basics/cs336_basics/bpe/output/owt-train_optim_vocab_32000.pkl\"\n",
    "owt_merges_path = \"/home/niu/code/cs336/assignment1-basics/cs336_basics/bpe/output/owt-train_optim_merges_32000.pkl\"\n",
    "\n",
    "owt_tokenizer = Tokenizer.from_files(\n",
    "    vocab_filepath=owt_vocab_path,\n",
    "    merges_filepath=owt_merges_path,\n",
    "    special_tokens=[\"<|endoftext|>\"],\n",
    ")\n",
    "\n",
    "with open(tiny_story_test_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    tiny_story_test = f.read(2000)  # 只读取前500个字符\n",
    "para_tiny_story_ids = owt_tokenizer.encode_parallel(tiny_story_test, os.cpu_count())\n",
    "compression_ratio = len(list(tiny_story_test.encode(\"utf-8\"))) / len(para_tiny_story_ids)\n",
    "print(f\" owt tokenizer on tiny story dataset compression_ratio is {compression_ratio}\")\n",
    "\n",
    "\n",
    "owt_test_path = \"/home/niu/code/cs336/assignment1-basics/data/owt_valid.txt\"\n",
    "\n",
    "\n",
    "Tiny_story_vocab_path = \"/home/niu/code/cs336/assignment1-basics/cs336_basics/bpe/output/TinyStoriesV2-GPT4-train_optim_vocab_10000.pkl\"\n",
    "Tiny_story_merges_path = \"/home/niu/code/cs336/assignment1-basics/cs336_basics/bpe/output/TinyStoriesV2-GPT4-train_optim_merges_10000.pkl\"\n",
    "\n",
    "\n",
    "tiny_story_tokenizer = Tokenizer.from_files(\n",
    "    vocab_filepath=Tiny_story_vocab_path,\n",
    "    merges_filepath=Tiny_story_merges_path,\n",
    "    special_tokens=[\"<|endoftext|>\"],\n",
    ")\n",
    "\n",
    "with open(owt_test_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    owt_test = f.read(2000)  # 只读取前500个字符\n",
    "\n",
    "para_owt_ids = tiny_story_tokenizer.encode_parallel(owt_test, os.cpu_count())\n",
    "compression_ratio = len(list(owt_test.encode(\"utf-8\"))) / len(para_owt_ids)\n",
    "print(f\" tiny_story tokenizer on owt dataset compression_ratio is {compression_ratio}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c3b087",
   "metadata": {},
   "source": [
    "# Estimate the throughput of your tokenizer (e.g., in bytes/second). How long would it take to tokenize the Pile dataset (825GB of text) 也就是测一下tokenize的速度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc88c810",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding text...: 100%|██████████| 12915/12915 [09:49<00:00, 21.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenize time cost is 589.8910937309265 seconds, \n",
      "       throughput is 2198.3261211797703 tokens/second\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "sub processs Encoding chunk...: 100%|██████████| 6458/6458 [05:05<00:00, 21.14it/s]\n",
      "sub processs Encoding chunk...: 100%|██████████| 6457/6457 [05:06<00:00, 21.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parallel tokenize time cost is 306.2298786640167 seconds, \n",
      "       throughput is 4234.639041942632 tokens/second\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "Tiny_story_test_path = \"/home/niu/code/cs336/assignment1-basics/tests/fixtures/tinystories_sample_5M.txt\"\n",
    "\n",
    "Tiny_story_vocab_path = \"/home/niu/code/cs336/assignment1-basics/cs336_basics/bpe/output/TinyStoriesV2-GPT4-train_optim_vocab_10000.pkl\"\n",
    "Tiny_story_merges_path = \"/home/niu/code/cs336/assignment1-basics/cs336_basics/bpe/output/TinyStoriesV2-GPT4-train_optim_merges_10000.pkl\"\n",
    "\n",
    "tiny_story_tokenizer = Tokenizer.from_files(\n",
    "    vocab_filepath=Tiny_story_vocab_path,\n",
    "    merges_filepath=Tiny_story_merges_path,\n",
    "    special_tokens=[\"<|endoftext|>\"],\n",
    ")\n",
    "tiny_story_test = open(Tiny_story_test_path, \"r\", encoding=\"utf-8\").read() \n",
    "\n",
    "\n",
    "\n",
    "start_time = time.time() \n",
    "tiny_story_ids = tiny_story_tokenizer.encode(tiny_story_test)\n",
    "end_time = time.time()\n",
    "print(f\"tokenize time cost is {end_time - start_time} seconds, \\n \\\n",
    "      throughput is {len(tiny_story_ids) / (end_time - start_time)} tokens/second\")\n",
    "\n",
    "# 并行\n",
    "start_time = time.time() \n",
    "tiny_story_ids = tiny_story_tokenizer.encode_parallel(tiny_story_test)\n",
    "end_time = time.time()\n",
    "print(f\"Parallel tokenize time cost is {end_time - start_time} seconds, \\n \\\n",
    "      throughput is {len(tiny_story_ids) / (end_time - start_time)} tokens/second\")\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa9a592",
   "metadata": {},
   "source": [
    "\n",
    "# 从上面的实验结果可以看出来，使用两个进程进行并行tokenize，吞吐量也就是两倍，嘿嘿   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3a2777",
   "metadata": {},
   "source": [
    "\n",
    "What happens if you tokenize your OpenWebText sample with the TinyStories tokenizer? Com-\n",
    "pare the compression ratio and/or qualitatively describe what happens.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2519dab0",
   "metadata": {},
   "source": [
    "# 这个意思是看看A训练数据集上面训练的tokenzier在B上效果如何，用压缩率来判断 (bytes/token)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eafd312a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tiny_story_path = \"/home/niu/code/cs336/assignment1-basics/tests/fixtures/tinystories_sample_5M.txt\"\n",
    "owt_path = \"/home/niu/code/cs336/assignment1-basics/data/owt_valid.txt\"\n",
    "\n",
    "\n",
    "Tiny_story_vocab_path = \"/home/niu/code/cs336/assignment1-basics/cs336_basics/bpe/output/TinyStoriesV2-GPT4-train_optim_vocab_10000.pkl\"\n",
    "Tiny_story_merges_path = \"/home/niu/code/cs336/assignment1-basics/cs336_basics/bpe/output/TinyStoriesV2-GPT4-train_optim_merges_10000.pkl\"\n",
    "\n",
    "# tiny_tokenizer\n",
    "tiny_tokenizer = Tokenizer.from_files(\n",
    "    vocab_filepath=Tiny_story_vocab_path,\n",
    "    merges_filepath=Tiny_story_merges_path,\n",
    "    special_tokens=[\"<|endoftext|>\"],\n",
    ")\n",
    "\n",
    "\n",
    "owt_vocab_path = \"/home/niu/code/cs336/assignment1-basics/cs336_basics/bpe/output/OpenWebTextV2-GPT4-train_optim_vocab_32000.pkl\"\n",
    "owt_merges_path = \"/home/niu/code/cs336/assignment1-basics/cs336_basics/bpe/output/OpenWebTextV2-GPT4-train_optim_merges_32000.pkl\"\n",
    "\n",
    "# owt_tokenizer\n",
    "owt_tokenizer = Tokenizer.from_files(\n",
    "    vocab_filepath=owt_vocab_path,\n",
    "    merges_filepath=owt_merges_path,\n",
    "    special_tokens=[\"<|endoftext|>\"],\n",
    ")\n",
    "\n",
    "# 读取数据\n",
    "tiny_story_text = open(tiny_story_path, \"r\", encoding=\"utf-8\").read()\n",
    "owt_text = open(owt_path, \"r\", encoding=\"utf-8\").read()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs336-basics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
